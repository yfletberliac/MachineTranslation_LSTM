# Machine Translation using LSTM architecture for RNNs
## Personal DTU project - Translation for specialized texts

Standard recurrent neural network architectures challenged with translation tasks can only provide a limited range of context which is problematic for complex translations. I used for this project a Long Short-Term Memory architecture for Recurrent Neural Networks cells.
In the paper *machine_translation_report.pdf* I focus on: explaining the LSTM architecture for RNNs, describing the trained model and expressing some experiments.

#### The repository :
In the repository you can find the environment/setups needed to run the TensorFlow implementation of Sequence2Sequence Learning with LSTM (with GPU support) - you can also directly use my public EC2 AMI (id: **ami-12281d05**). Choosing a g2.8xlarge instance seems to be a good performance/price ratio compromise.
